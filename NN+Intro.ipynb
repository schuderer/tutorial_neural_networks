{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "# This assumes you have already seen the usual schematics and\n",
    "# diagrams that show up first when you google \"perceptron\" and\n",
    "# \"neural network\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My first perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I have this input set, consisting of three values\n",
    "input = [1, 0, 1]\n",
    "\n",
    "# and want this output\n",
    "target = 1\n",
    "\n",
    "# We have one perceptron node, with one incoming connection per\n",
    "# input value, where the incoming connections have some\n",
    "# random weights\n",
    "node = [0.1, 0.2, -0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When a perceptron is activated, it calculates an output value\n",
    "# by multiplying each input value by the weight of its \n",
    "# incoming connection, then summing them all up....\n",
    "def sum_prod(a_node, some_input):\n",
    "    the_sum = 0.0\n",
    "    for weight_index in range(len(a_node)):\n",
    "        the_sum += a_node[weight_index] * some_input[weight_index]\n",
    "    return the_sum;\n",
    "\n",
    "# ...then passing the resulting number through an activation function.\n",
    "# There are several possibilities for such a function, let's use\n",
    "# the most common one here: the sigmoid function\n",
    "def sigm(raw_val): \n",
    "    return 1.0/(1.0 + np.exp(-raw_val)) # sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's generate our first ever perceptron output!\n",
    "output = sigm(sum_prod(node, input))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hmm... not quite the output we wanted. Let's look at the error\n",
    "# 1/2 * sum_over_outputs(output - target)^2:\n",
    "error = 0.5 * (output - target) ** 2\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Some math. Skip if you want. #########\n",
    "# Let's see how much we should adjust the 3 weights. We will\n",
    "# differentiate the error function for each weight w_i to know \n",
    "# which way to adjust the weights so that the error gets smaller.\n",
    "# I) expand d(0.5 *(output - target)^2)/dw_i to\n",
    "#    d(0.5 *(sigm(sum_prod(ws,inputs)) - target)^2)/dw_i\n",
    "# II) Use chain rule on d(...)^2/dw_i --> 2(...) * d(...)/dw_i:\n",
    "#    (sigm(sum_prod(ws,inputs)) - target) \n",
    "#           * d(sigm(sum_prod(ws,inputs)) - target)/dw_i\n",
    "# III) \"target\" in the second term is a constant and disappears\n",
    "#    (sigm(sum_prod(ws,inputs)) - target) \n",
    "#           * d(sigm(sum_prod(ws,inputs)))/dw_i\n",
    "# IV) Use chain rule on d(sigm(...))/dw_i --> d(sigm())/w_i * d(...)/dw_i\n",
    "#     This results in the derivative of the activation function and\n",
    "#     another term:\n",
    "#    (sigm(sum_prod(ws,inputs)) - target) \n",
    "#           * output * (1- output)\n",
    "#           * d(sum_prod(ws,inputs))/dw_i\n",
    "# V) A nice boon is that all the terms of the error sum (except the \n",
    "#    one containing the weight we deal with) are treated as constants\n",
    "#    and becoming zeroes, disappear from our derivative:\n",
    "#    d(i_1*w_1 + i_2*w_2 + i_3*w_3)/dw_2 = 0 + i_2 + 0 = i_2\n",
    "#\n",
    "#    (sigm(sum_prod(ws,inputs)) - target) \n",
    "#           * output * (1- output)\n",
    "#           * input_i\n",
    "# VI) We already know the value of sigm(sum_prod(ws,inputs))\n",
    "#     In the formula -- it's our output value. So our derivate is:\n",
    "# \n",
    "#    (output - target) * (output * (1- output)) * input_i\n",
    "#### Continue here. #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So let's get to tweaking our weights. By what amount should\n",
    "# we adjust them and in what direction.\n",
    "# The answer is: by the slope of our error 0.5 *(output - target)^2\n",
    "# Lets' calculate that in code.\n",
    "# First step, what's the outer function derived?\n",
    "diff = output - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# But this is not the whole picture: When calculating the derivative\n",
    "# of the error 1/2*(output - target)^2, we need to apply the chain\n",
    "# rule, multiplying \"diff\" with the derivative of output (because\n",
    "# output also depends on the weights).\n",
    "# Where did \"output\" come from? That's right, we need to get the\n",
    "# derivative of the activation function:\n",
    "def sigm_der(output):\n",
    "    return output * (1- output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Putting the last two steps together, we get an important function\n",
    "# of backpropagation:\n",
    "def delta(output, target):\n",
    "    return (output - target) * sigm_der(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The last step of getting the adjustments for each individual weight is \n",
    "# multiplying what we have so for with the derivative of the multiplication\n",
    "# of each input value with their corresponding weight, d(sumProd)/dw_i.\n",
    "# Deriving input_i * weight_i for weight_i, yields, unsurprisingly, the input_i.\n",
    "# Now multiply this value with each weight's input, and we get how much\n",
    "# we should adjust that weight:\n",
    "def adjusted_weights(a_node, node_inputs, output, target):\n",
    "    node_copy = a_node[:]\n",
    "    for weight_index in range(len(node_copy)):\n",
    "        slope_climb = delta(output, target) * node_inputs[weight_index]\n",
    "        slope_descent_step = - (0.1 * slope_climb)\n",
    "        node_copy[weight_index] += slope_descent_step\n",
    "    return node_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now learn, my pretty, learn!\n",
    "node = adjusted_weights(node, input, output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now calculate the perceptron's output again with the adjusted weights!\n",
    "output2 = sigm(sum_prod(node, input))\n",
    "print(str(output2) + \" is closer to \" + str(target) + \" than \" + str(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: logical AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Great! Have you already learned something? Because your perceptron has!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's apply this over and over, with not just one set of inputs,\n",
    "# but some more data:\n",
    "inputs = [\n",
    " [1, 0, 1],\n",
    " [0, 1, 1],\n",
    " [1, 1, 1],\n",
    " [0, 0, 1]\n",
    "]\n",
    "\n",
    "# We want to output 1 when our first and second input value are both 1,\n",
    "# 0 otherwise (logical AND)\n",
    "targets_AND = [0, 0, 1, 0]\n",
    "\n",
    "# I'm lazy, so let's use the same \"random\" initial weights again\n",
    "initial_node = [0.1, 0.2, -0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let 'er rip\n",
    "def train(num_epochs, a_node, some_inputs, some_targets):\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        err_sum = 0.0\n",
    "        for input_idx in range(len(some_inputs)):\n",
    "            curr_input = inputs[input_idx]\n",
    "            output = sigm(sum_prod(a_node, curr_input))\n",
    "            a_node = adjusted_weights(a_node, curr_input, output, some_targets[input_idx])\n",
    "            err_sum += 0.5 * (output - some_targets[input_idx]) ** 2\n",
    "        if (epoch_idx % 200 == 0):\n",
    "            print(\"Iteration \"+str(epoch_idx)+\" Error: \"+str(err_sum / 100))\n",
    "            err_sum = 0.0\n",
    "    return a_node\n",
    "\n",
    "trained_node_AND = train(2000, initial_node, inputs, targets_AND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Of course we're now using the error on the same data that we train on...\n",
    "# But hey, live dangerously!\n",
    "\n",
    "# Let's try some values to test:\n",
    "print(sigm(sum_prod(trained_node_AND, [1,0,1]))) # Zero-ish, kinda\n",
    "print(sigm(sum_prod(trained_node_AND, [1,1,1]))) # One-ish\n",
    "print(sigm(sum_prod(trained_node_AND, [0,0,1]))) # Even more zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can of make the node learn much faster, e.g. by tuning the\n",
    "# learning rate and/or starting with a higher learning rate which\n",
    "# decreases with the number of iterations (simulated annealing),\n",
    "# or using more ideas such as momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can also learn other stuff like logical OR (we can re-use the same inputs):\n",
    "targets_OR = [1, 1, 1, 0]\n",
    "\n",
    "trained_node_OR = train(2000, initial_node, inputs, targets_OR)\n",
    "\n",
    "print(sigm(sum_prod(trained_node_OR, [1,0,1]))) # One-ish, kinda\n",
    "print(sigm(sum_prod(trained_node_OR, [0,1,1]))) # Same\n",
    "print(sigm(sum_prod(trained_node_OR, [0,0,1]))) # Somewhat zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok, now you now exactly how a perceptron works.\n",
    "# So where to go from here?\n",
    "\n",
    "# First, you should know why there was always that third input value\n",
    "# lurking around, always being 1. It's called a bias. \n",
    "# Without it, the output value 0 (or all-zero input values) could get\n",
    "# us stuck (the descent would be 0, too).\n",
    "# Nodes have a weight for the bias, just like for any other input. You can\n",
    "# imagine it a bit like the intercept in a linear function.\n",
    "\n",
    "# In most cases, you provide every layer in a multilayer perceptron\n",
    "# with a bias. We'll do so from here on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Fail & Moar neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So we've got a one-neuron perceptron, which is somewhat like a neural net\n",
    "# already, right? Let's make it play Go!\n",
    "\n",
    "# Just kidding, let's try it on logical XOR (exclusive OR) first\n",
    "# and give it more iterations to make sure it learns well:\n",
    "\n",
    "targets_XOR = [1, 1, 0, 0]\n",
    "\n",
    "trained_node_XOR = train(6000, initial_node, inputs, targets_XOR)\n",
    "\n",
    "print(sigm(sum_prod(trained_node_XOR, [1,0,1]))) # Hmmm, should be 1\n",
    "print(sigm(sum_prod(trained_node_XOR, [0,1,1]))) # Same here\n",
    "print(sigm(sum_prod(trained_node_XOR, [0,0,1]))) # Should have been 0\n",
    "print(sigm(sum_prod(trained_node_XOR, [1,1,1]))) # Same, hard to say\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So we trained a completely useless perceptron that always returs 1/2\n",
    "# instead of 0 or 1.\n",
    "# I'd call that an outspokenly indecisive perceptron (TM).\n",
    "\n",
    "# Of course you guessed it: We need more power = more neurons.\n",
    "\n",
    "# The most common neural networks are called multilayer perceptrons\n",
    "# and are organized as fully-connected feed-forward networks.\n",
    "# This means that they have M distinctive layers of 1-N perceptrons each\n",
    "# and the output of every node of layer 1 acts as input of each of the \n",
    "# neurons in the following layer 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a simple 2-layer network (we'll use numpy to\n",
    "# create truly random nodes).\n",
    "# ...\n",
    "# ....Also, for good measure, let's pass the activation function\n",
    "# and it's derivative as well, so the layer knows them and doesn't have\n",
    "# to rely on too many globally defined functions....\n",
    "def create_layer(num_nodes, num_inputs, uses_bias, act_func, act_der_func, learningrate):\n",
    "    num_weights = num_inputs + (1 if uses_bias else 0)\n",
    "    nodes = 2*np.random.random((num_nodes, num_weights))-1 # *2-1 to center around 0\n",
    "    #nodes = np.random.random((num_nodes, num_weights))/num_nodes + 0.001 # 0.0001-0.01\n",
    "    return (nodes, act_func, act_der_func, learningrate)\n",
    "\n",
    "layer1 = create_layer(num_nodes=2, num_inputs=2, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1)\n",
    "# results in something like\n",
    "# [[ 0.35335893, -0.52181256,  0.20123706],  # first node with weights\n",
    "#  [-0.93498546,  0.31766613, -0.15244034]] # second node with weights\n",
    "\n",
    "layer2 = create_layer(num_nodes=1, num_inputs=2, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1)\n",
    "# results in something like\n",
    "# [[-0.32673713,  0.3158073 ,  0.78059177]] # only one node in this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll create outputs per layer with a function that can handle a whole\n",
    "# layer at a time and an input set and returns the output of all the nodes\n",
    "# of that layer.\n",
    "def layer_output(a_layer, input_set):\n",
    "    nodes = a_layer[0]\n",
    "    act_func = a_layer[1]\n",
    "    uses_bias = len(nodes[0]) == len(input_set) + 1\n",
    "    biased_inputs = np.append(input_set, 1) if uses_bias else input_set\n",
    "    sumprods = np.dot(nodes, biased_inputs) # np.dot() = loop of sum_prod()\n",
    "    # results in something like array([ 2.01576287,  1.51741144])\n",
    "    # 2 nodes, 2 sumprods, two outputs (numpy applies the function to every \n",
    "    # sumprod automatically):\n",
    "    return act_func(sumprods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's test if it works:\n",
    "print(layer_output(layer1, [1,0]))\n",
    "# [ 0.63520124  0.25210333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now make it neat and put the network into one variable\n",
    "network = [\n",
    "    create_layer(num_nodes=2, num_inputs=2, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1), # layer 1 (input layer)\n",
    "    create_layer(num_nodes=1, num_inputs=2, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1) # layer 2 (output layer)  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A function to loop over the layers and feed output forward from one layer\n",
    "# to the next.\n",
    "def network_output(a_network, input_set):\n",
    "    curr_data = input_set\n",
    "    for curr_layer in a_network:\n",
    "        curr_data = layer_output(curr_layer, curr_data)\n",
    "    return curr_data\n",
    "\n",
    "print(network_output(network, [1,0]))\n",
    "# Results in something like\n",
    "# [ 0.40050464] # output in a list because the last layer also can have several nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've already got some pretty generic (read: powerful) functions here. Now let's\n",
    "# also make multilayer networks learn. Recall that we calculated the weight\n",
    "# adjustments for one single perceptron with the formula:\n",
    "\n",
    "#     (output - target) * act_der(output) * input_i * (-0.1)\n",
    "\n",
    "# This still applies for our last layer. But what about the previous layer,\n",
    "# which has an output, but doesn't have an explicit target (we only have\n",
    "# a defined target for the last layer).\n",
    "# Well, \"output - target\" means how much our output is off, and as it turns\n",
    "# out, one layer's output is the next layer's input. And when our next layer's\n",
    "# input weight's have to be adjusted up or down, THIS is actually our layer's\n",
    "# \"off-by-x\" amount.\n",
    "\n",
    "# So, in our formula for non-output layers, we replace \"output - target\" by\n",
    "# the \"act_der(output) * input_i\" of the next layer (I'll call this \"diff\"):\n",
    "\n",
    "#     next_layers_diff * act_der(output) * input_i * (-0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to adjust the weights of all the nodes in a layer\n",
    "# (it still uses inputs and outputs, but not for calculating the diffs):\n",
    "def adjust_layer(a_layer, some_inputs, outputs, output_diffs):\n",
    "    nodes = a_layer[0]\n",
    "    act_dev_func = a_layer[2]\n",
    "    learningrate = a_layer[3]\n",
    "    # Remember: inputs, outputs and output_diffs are 1D-numpy arrays.\n",
    "    # nodes is a 2D numpy array (rows = nodes, cols = weights)\n",
    "    uses_bias = len(nodes[0]) == len(some_inputs) + 1\n",
    "    biased_inputs = np.append(some_inputs, 1) if uses_bias else some_inputs\n",
    "    # do what our delta function did previously, only for all nodes in one step:\n",
    "    deltas = output_diffs * act_dev_func(outputs)\n",
    "    \n",
    "    # [d1, d2] --> [[d1],   necessary for np.dot product\n",
    "    #               [d2]]\n",
    "    deltas_transposed = np.expand_dims(deltas, axis=1)\n",
    "    \n",
    "    # all deltas (one for each output/node) multiplied by all inputs:\n",
    "    gradient_climb = deltas_transposed.dot([biased_inputs]) # dot = loop of sumProds\n",
    "    gradient_descent_step = -learningrate * gradient_climb\n",
    "    # [[w1, w2, w3], # node 1 weight changes\n",
    "    #  [w1, w2, w3]] # node 2 weight changes\n",
    "    \n",
    "    # gradient_descent_step has the same dimensions as our layer, so we can just\n",
    "    # add the steps to the weights in one operation:\n",
    "    nodes += gradient_descent_step\n",
    "    \n",
    "    # Return the weight \"off-bys\". These will be our \"next_layers_diff\":\n",
    "    return deltas.dot(nodes[:,0:len(some_inputs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try it.\n",
    "print(\"Uncorrected weights of output layer:\")\n",
    "print(layer2[0])\n",
    "\n",
    "# Adjust weights for all nodes in that layer once:\n",
    "adjust_layer(layer2, [1, 0], np.asarray([0.9]), [ 0.9 - 0 ])\n",
    "\n",
    "print(\"Result of adjustment:\")\n",
    "print(layer2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now package this into a neat little function to loop over all the layers\n",
    "# and propagate errors back from layer to layer.\n",
    "# In order to avoid having to transfer a lot of inputs/outputs between functions,\n",
    "# here we will first apply every layer forward, then do backpropagation with the\n",
    "# results.\n",
    "def adjust_network(a_network, input_set, targets):\n",
    "    # forward run to obtain necessary inputs/outputs, layer by layer\n",
    "    input_per_layer = []\n",
    "    output_per_layer = []\n",
    "    curr_data = input_set\n",
    "    for curr_layer in a_network:\n",
    "        input_per_layer.append(curr_data)\n",
    "        curr_data = layer_output(curr_layer, curr_data)\n",
    "        output_per_layer.append(curr_data)\n",
    "    # backward run with backprop:\n",
    "    # start with calculating \"diff\" a.k.a. \"off-by\" of output layer:\n",
    "    last_outputs = output_per_layer[len(a_network)-1]\n",
    "    output_diffs = last_outputs - targets\n",
    "    for layer_index in reversed(range(len(a_network))):\n",
    "        #print(\"layer \"+str(layer_index))\n",
    "        #print(\"input: \")\n",
    "        #print(input_per_layer[layer_index])\n",
    "        #print(\"output\")\n",
    "        #print(output_per_layer[layer_index])\n",
    "        #print(\"output-diffs\")\n",
    "        #print(output_diffs)\n",
    "        curr_layer = a_network[layer_index]\n",
    "        curr_inputs = input_per_layer[layer_index]\n",
    "        curr_outputs = output_per_layer[layer_index]\n",
    "        output_diffs = adjust_layer(curr_layer, curr_inputs, curr_outputs, output_diffs)\n",
    "    # Let's return our actual network output so have something to look at\n",
    "    return last_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ok, let's see if this works:\n",
    "# Original network\n",
    "print(network)\n",
    "# Do 1 learning iteration with some hard-coded values:\n",
    "adjust_network(network, [1, 0], [0])\n",
    "# Changed network:\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A new train function for multilayer networks\n",
    "def train(num_epochs, a_network, some_inputs, some_targets):\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        err_sum = 0.0\n",
    "        for input_idx in range(len(some_inputs)):\n",
    "            curr_input = some_inputs[input_idx]\n",
    "            curr_target = some_targets[input_idx]\n",
    "            output = adjust_network(a_network, curr_input, curr_target)\n",
    "            err_sum += np.mean(0.5 * (output - curr_target) ** 2)\n",
    "            iteration = epoch_idx*len(some_inputs) + input_idx\n",
    "            if (iteration % 1000 == 0):\n",
    "                print(\"Epoch \"+str(epoch_idx)+\", Iteration \"+str(iteration)+\" Error: \"+str(err_sum / 100))\n",
    "                err_sum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try again with XOR:\n",
    "inputs = [\n",
    " [1, 0],\n",
    " [0, 1],\n",
    " [1, 1],\n",
    " [0, 0]\n",
    "]\n",
    "targets_XOR = [1, 1, 0, 0]\n",
    "\n",
    "network = [\n",
    "    create_layer(num_nodes=3, num_inputs=2, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1), # layer 1 (input layer - we need 3 nodes here)\n",
    "    create_layer(num_nodes=1, num_inputs=3, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1) # layer 2 (output layer)  \n",
    "]\n",
    "\n",
    "train(6000, network, inputs, targets_XOR)\n",
    "\n",
    "print(network_output(network, [1,0])) # 1-ish\n",
    "print(network_output(network, [0,1])) # At least somewhat closer to 1 than to 0\n",
    "print(network_output(network, [1,1])) # Goes into the 0 direction, looks about right\n",
    "print(network_output(network, [0,0])) # Zero. Close enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok, nice...\n",
    "# But was all that necessary just for XOR? -- Of course not.\n",
    "# Can it play Go now? -- Still not very well, but it can do some\n",
    "# other nifty things already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST Digit Handwriting Recognition\n",
    "def read_mnist(file_name):\n",
    "    mnist_train = np.genfromtxt(file_name, delimiter=',')\n",
    "    # get inputs (pixels) and scale them to be between (somewhat) 0 and (nearly) 1\n",
    "    mnist_inputs = mnist_train[:,1:] / (255.0 * 1.005) + 0.001\n",
    "    # get outputs (numbers) as 0...9 one-hot vectors\n",
    "    mnist_outnums = np.int_(mnist_train[:,0])\n",
    "    mnist_targets = np.zeros((len(mnist_train), 10))\n",
    "    mnist_targets[np.arange(len(mnist_train)), mnist_outnums] = 1.0\n",
    "    return (mnist_inputs, mnist_targets)\n",
    "\n",
    "def print_bitmap(pixels, width):\n",
    "    levels = \".:+I#\"\n",
    "    line = \"\"\n",
    "    for p in pixels:\n",
    "        lev_idx = int(p * len(levels))\n",
    "        #print(str(int(p)) + \", \"+str(lev_idx))\n",
    "        line += levels[lev_idx]\n",
    "        if (len(line) % width == 0):\n",
    "            print(line)\n",
    "            line = \"\"\n",
    "\n",
    "mnist_inputs, mnist_targets = read_mnist('mnist_train_10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The rest is just the same as before, just with a few more neurons\n",
    "mnist_network = [\n",
    "    create_layer(num_nodes=200, num_inputs=784, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1), # layer 1 (input layer - one input per pixel)\n",
    "    create_layer(num_nodes=10, num_inputs=200, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1) # layer 2 (output layer)  \n",
    "]\n",
    "\n",
    "train(15, mnist_network, mnist_inputs, mnist_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's test it\n",
    "mnist_test_inputs, mnist_test_targets = read_mnist('mnist_test_1000.csv')\n",
    "\n",
    "def test_mnist(network):\n",
    "    num_correct = 0.0\n",
    "    for idx in range(len(mnist_test_inputs)):\n",
    "        out_vec = np.around(network_output(network, mnist_test_inputs[idx]))\n",
    "        out_num = np.int_(out_vec.dot(np.arange(10)))\n",
    "        targ_num = np.int_(mnist_test_targets[idx].dot(np.arange(10)))\n",
    "        num_correct += 1.0 if out_num == targ_num else 0.0\n",
    "        if (rand.randint(0,50) == 0):\n",
    "            result = \"correct\" if out_num == targ_num else \"WRONG  \"\n",
    "            print(result + \": \"+str(out_num)+\" vs. \"+str(targ_num))\n",
    "            if (out_num != targ_num):\n",
    "                print_bitmap(mnist_test_inputs[idx], 28)\n",
    "    acc = num_correct/len(mnist_test_inputs)\n",
    "    print(str(int(acc*100))+\"% correctly recognized.\")\n",
    "\n",
    "test_mnist(mnist_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, it reaches some 80-something percent accuracy pretty fast\n",
    "# and eventually flattens out at about 88%\n",
    "\n",
    "# We can add changes which lead to significant improvements in accuracy,\n",
    "# of which many are quite different, such as adding convolutional layers and pooling\n",
    "# layers (leaving the fully-connected paradigm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can still experiment with simply adding more layers, though, and see what\n",
    "# happens:\n",
    "\n",
    "mnist_network2 = [\n",
    "    create_layer(num_nodes=200, num_inputs=784, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1), # layer 1 (input layer - one input per pixel)\n",
    "    create_layer(num_nodes=100, num_inputs=200, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1), # layer 2 (hidden layer)\n",
    "    create_layer(num_nodes=10, num_inputs=100, uses_bias=True, act_func=sigm, act_der_func=sigm_der, learningrate=0.1) # layer 3 (output layer)  \n",
    "]\n",
    "\n",
    "train(15, mnist_network2, mnist_inputs, mnist_targets)\n",
    "\n",
    "test_mnist(mnist_network2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
